{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex macros:\n",
    "$$\n",
    "\\newcommand{\\E}{\\text{E}}\n",
    "\\newcommand{\\mbf}{\\mathbf}\n",
    "\\newcommand{\\bo}{\\mathbf}\n",
    "\\newcommand{\\bs}{\\boldsymbol}\n",
    "\\newcommand{\\Var}{\\text{Var}}\n",
    "\\newcommand{\\Cov}{\\text{Cov}}\n",
    "\\newcommand{\\e}{\\frac{1}{\\sigma^2_e}}\n",
    "\\newcommand{\\f}{\\frac{1}{\\sigma^2_{\\alpha}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    MathJax.Hub.Config({\n",
       "      TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "    });\n",
       "    MathJax.Hub.Queue( \n",
       "        [\"resetEquationNumbers\",MathJax.InputJax.TeX], \n",
       "        [\"PreProcess\",MathJax.Hub], \n",
       "        [\"Reprocess\",MathJax.Hub] \n",
       "    );\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro javascript_str(s) display(\"text/javascript\", s); end\n",
    "javascript\"\"\"\n",
    "    MathJax.Hub.Config({\n",
    "      TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "    });\n",
    "    MathJax.Hub.Queue( \n",
    "        [\"resetEquationNumbers\",MathJax.InputJax.TeX], \n",
    "        [\"PreProcess\",MathJax.Hub], \n",
    "        [\"Reprocess\",MathJax.Hub] \n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative methods for solving linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the system of consistent linear equations: \n",
    "\n",
    "$$\n",
    "\\mathbf{Ax} = \\mathbf{b},\n",
    "$$\n",
    "Three iterative methods that we will use for solving the linear systems are the Jacobi method, the Gauss-Seidel Methods, and the Preconditioned Conjugate Gradient (PCCG) method. These methods can be used to solve normal equations shown in the previous section and Mixed Model Equations (MME) we will covered later. Consider MME, the left-hand-side (LHS) of the MME is represented by $\\mathbf{A}$ and the right-hand-side (RHS) by $\\mathbf{b}$. The LHS of the MME is often too large to store in memory as a “fully-stored” matrix. However, $\\mathbf{A}$ is often very sparse. Thus, it is may be possible to store only the non-zero elements of $\\mathbf{A}$ and compute $\\mathbf{Ax_n}$, using sparse matrix methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conjugate gradient method, the solution at iteration $n+1$ is: \n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha_n\\mathbf{d}_n, \n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\label{eq:5}\n",
    "\\alpha_n = \\frac{-\\mathbf{r}_n'\\mathbf{r}_n}{\\mathbf{d}_n'\\mathbf{Ad}_n},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  \\label{eq:6}\n",
    "  \\mathbf{r}_n = \\mathbf{Ax}_n - \\mathbf{b},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  \\label{eq:7}\n",
    "  \\mathbf{d}_n = \\mathbf{r}_n - \\beta_{n-1}\\mathbf{d}_{n-1}, \n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "  \\label{eq:8}\n",
    "  \\beta_{n-1} = \\frac{-\\mathbf{r}_n'\\mathbf{r}_n}\n",
    "                {\\mathbf{r}'_{n-1}\\mathbf{r}_{n-1}}.\n",
    "\\end{equation}\n",
    "\n",
    "It can be shown that the residual can be computed as\n",
    "\\begin{equation}\n",
    "  \\label{eq:9}\n",
    "  \\mathbf{r}_n = \\mathbf{r}_{n-1} + \\alpha_{n-1}\\mathbf{Ad}_{n-1},\n",
    "\\end{equation}\n",
    "\n",
    "and thus avoiding computation of $\\mathbf{Ax}_n$. However, using (\\ref{eq:9})\n",
    "leads to the accumulation of rounding errors. Thus, it is recommended that (\\ref{eq:6}) is used every 50 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Jacobi method, it is not intuitively obvious why the conjugate \n",
    "gradient method works. Following is an attempt to explain why the method works.\n",
    "\n",
    "In the conjugate gradient method, the value of $\\bo{x}$ that minimizes\n",
    "\\begin{equation}\n",
    "\\label{eq:10}\n",
    "\\begin{split}\n",
    "f(\\bo{x}) &= \\frac{1}{2}\\bo{x}'\\bo{Ax} -\\bo{b}'\\bo{x}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "is obtained by line minimizations in $n$ linearly independent directions, where\n",
    "$n$ is the order of the symmetric positive definite matrix $\\bo{A}$. Note that\n",
    "after minimization of $f(\\bo{x})$ in any direction $\\bo{d}_i$, the gradient\n",
    "$\\bo{r}_{i+1}$ will be orthogonal to $\\bo{d}_i$. \n",
    "\n",
    "In the conjugate gradient method, each direction $\\bo{d}_i$ is is chosen such\n",
    "that the gradient $\\bo{r}_{i+1}$ will also be orthogonal to all the directions\n",
    "$\\bo{d}_j$, for $j<i$, that have already been used for line minimization. If the\n",
    "directions are also linearly independent, after $n$ line minimizations, the\n",
    "function will be at its minimum in $n$ linearly independent directions. Further,\n",
    "at this point, the gradient \n",
    "$$\n",
    "\\bo{r} = \\bo{Ax} - \\bo{b}\n",
    "$$\n",
    "is orthogonal to the $n$ direction vectors. Thus, it must be the zero vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a description of how the direction vectors are computed.  Suppose\n",
    "the search is initiated at $\\bo{x}_0 = \\bo{0}$. At this point, the gradient of\n",
    "the $f(\\bo{x})$ is\n",
    "\\begin{equation}\n",
    "\\label{eq:11}\n",
    "  \\begin{split}\n",
    "    \\bo{r}_0 &= \\bo{Ax}_o - \\bo{b}\\\\\n",
    "            &= -b. \n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "Let $\\bo{d}_0 = \\bo{r}_0$ be the first direction for line minimization. After\n",
    "minimization of $f(\\bo{x})$ in the direction $\\bo{d}_0$, the value of $\\bo{x}$\n",
    "is\n",
    "\\begin{equation}\n",
    "  \\label{eq:12}\n",
    "  \\bo{x}_1 = \\bo{x}_0 + \\alpha_0\\bo{d}_0.\n",
    "\\end{equation}\n",
    "At $\\bo{x}_1$, the gradient of the function is\n",
    "\\begin{equation}\n",
    "  \\label{eq:13}\n",
    "  \\begin{split}\n",
    "    \\bo{r}_1 &= \\bo{Ax}_1 - \\bo{b}\\\\\n",
    "            &= \\bo{A}(\\bo{x}_0 + \\alpha_0\\bo{d}_0) - \\bo{b}\\\\\n",
    "            &= \\bo{r}_0 + \\alpha_0\\bo{Ad}_0,\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "and $\\bo{r}_1$ is orthogonal to $\\bo{d}_0$. Writing the product\n",
    "$\\bo{d}_0\\bo{r}_1=0$ as \n",
    "\\begin{equation}\n",
    "  \\label{eq:14}\n",
    "  \\begin{split}\n",
    "  \\bo{d}_0'\\bo{r}_1 &= \\bo{d}_0'\\bo{r}_0 + \\alpha_0\\bo{d}_0'\\bo{Ad}_0\\\\\n",
    "                 &= 0\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "shows that \n",
    "\\begin{equation}\n",
    "  \\label{eq:15}\n",
    "  \\alpha_0 = \\frac{- \\bo{d}'_0\\bo{r}_0}{\\bo{d}_0'\\bo{Ad}_0},\n",
    "\\end{equation}\n",
    "and in general,\n",
    "\\begin{equation}\n",
    "  \\label{eq:16}\n",
    "    \\alpha_i = \\frac{- \\bo{d}_i'\\bo{r}_i}{\\bo{d}_i'\\bo{Ad}_i}. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, line minimization\n",
    "proceeds in the direction $\\bo{d}_1$, and the value of $\\bo{x}$ at the minimum\n",
    "is\n",
    "\\begin{equation}\n",
    "\\label{eq:17}\n",
    "\\bo{x}_2 = \\bo{x}_1 + \\alpha_1\\bo{d}_1.\n",
    "\\end{equation}\n",
    "The gradient of the function at $\\bo{x}_2$ is \n",
    "\\begin{equation}\n",
    "\\label{eq:18}\n",
    "\\begin{split}\n",
    "\\bo{r}_2 &= \\bo{Ax}_2 - \\bo{b}\\\\ \n",
    "         &= \\bo{r}_1 + \\alpha_1\\bo{Ad}_1, \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "and $\\bo{r}_2$ is orthogonal to $\\bo{d}_1$. In the conjugate gradient method,\n",
    "the direction $\\bo{d}_1$ is chosen such that $\\bo{r}_2$ will also be orthogonal\n",
    "to the direction $\\bo{d}_0$. Thus the product \n",
    "\\begin{equation}\n",
    "  \\label{eq:19}\n",
    "  \\begin{split}\n",
    "    \\bo{d}_0'\\bo{r}_2 &= \\bo{d}_0'\\bo{r}_1 + \\alpha_1\\bo{d}_0'\\bo{Ad}_1\\\\\n",
    "                    &= \\alpha_1\\bo{d}_0'\\bo{Ad}_1\\\\\n",
    "                    &= 0.\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "So, the direction, $\\bo{d}_1$ must satisfy the condition\n",
    "$$\n",
    "\\bo{d}_0'\\bo{Ad}_1 = 0\n",
    "$$\n",
    "in order for $\\bo{r}_2$ to be orthogonal to $\\bo{d}_0$. To accomplish this,\n",
    "following the Grahm-Schmidt procedure, $\\bo{d}_1$ is written as\n",
    "\\begin{equation}\n",
    "  \\label{eq:20}\n",
    "  \\bo{d}_1 = \\bo{r}_1 - \\beta_{10}\\bo{d}_0.\n",
    "\\end{equation}\n",
    "Writing $\\bo{d}_0'\\bo{Ad}_1=0$ as\n",
    "\\begin{equation}\n",
    "  \\label{eq:21}\n",
    "  \\begin{split}\n",
    "   \\bo{d}_0'\\bo{Ad}_1 &= \\bo{d}_0'\\bo{Ar}_1 - \\beta_{10}\\bo{d}_0'\\bo{Ad}_0\\\\\n",
    "                    &= 0         \n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "shows that \n",
    "\\begin{equation}\n",
    "  \\label{eq:22}\n",
    "  \\beta_{10} = \\frac{\\bo{d}_0'\\bo{Ar}_1}{\\bo{d}_0'\\bo{Ad}_0}.\n",
    "\\end{equation}\n",
    "In general, \n",
    "\\begin{equation}\n",
    "  \\label{eq:23}\n",
    "  \\bo{r}_{i+1} = \\bo{r}_i + \\alpha_i\\bo{Ad}_i,\n",
    "\\end{equation}\n",
    "and by induction, $\\bo{r}_{i+1}'\\bo{d}_j=0$ provided $\\bo{d}_i'\\bo{Ad}_j=0$ for\n",
    "$j<i$. This can be achieved by using Grahm-Schmidt as \n",
    "\\begin{equation}\n",
    "  \\label{eq:24}\n",
    "  \\bo{d}_i = \\bo{r}_i - \\sum_{j=0}^{i-1} \\beta_{ij}\\bo{d}_j,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  \\label{eq:25}\n",
    "   \\beta_{ij} = \\frac{\\bo{d}_j'\\bo{Ar}_{i}}{\\bo{d}_j'\\bo{Ad}_j}.\n",
    "\\end{equation}\n",
    "Note that using the result $\\bo{r}_i\\bo{d}_j=0$ for $j<i$ in \\eqref{eq:24}\n",
    "implies:\n",
    "\\begin{equation}\n",
    "  \\label{eq:26}\n",
    "  \\bo{r}_i'\\bo{r}_j=0\\, \\text{for}\\,  j<i.\n",
    "\\end{equation}\n",
    "Using (\\ref{eq:26}) in (\\ref{eq:23}), shows that $\\bo{r}_i'\\bo{Ad}_j=0$ for\n",
    "$j<i-1$.  Thus, in (\\ref{eq:25}), $\\beta_{ij}=0$ for $j<i-1$, and the general\n",
    "expression for $\\bo{d}_i$ simplifies to\n",
    "\\begin{equation}\n",
    "  \\label{eq:27}\n",
    "  \\bo{d}_i = \\bo{r}_i - \\beta_{i-1}\\bo{d}_{i-1},\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  \\label{eq:28}\n",
    "  \\beta_{i-1} = \\frac{\\bo{d}_{i-1}'\\bo{Ar}_i}{\\bo{d}_{i-1}'\\bo{Ad}_{i-1}}.\n",
    "\\end{equation}\n",
    "Writing $\\bo{r}_i$ as \n",
    "\\begin{equation}\n",
    "  \\label{eq:29}\n",
    "  \\bo{r}_i = \\bo{r}_{i-1} + \\alpha_{i-1}\\bo{Ad}_{i-1}\n",
    "\\end{equation}\n",
    "and pre-multiplying by $\\bo{r}_i'$ gives\n",
    "\\begin{equation}\n",
    "  \\label{eq:30}\n",
    "  \\bo{r}_i'\\bo{r}_i = \\alpha_{i-1}\\bo{r}_i'\\bo{Ad}_{i-1}.\n",
    "\\end{equation}\n",
    "Pre-multiplying (\\ref{eq:29}) by $\\bo{d}_{i-1}$ gives\n",
    "\\begin{equation}\n",
    "  \\label{eq:31}\n",
    "  \\bo{d}_{i-1}'\\bo{r}_{i-1} = -\\alpha_{i-1}\\bo{d}_{i-1}'\\bo{Ad}_{i-1}.\n",
    "\\end{equation}\n",
    "Further, from (\\ref{eq:27}), we can see that \n",
    "\\begin{equation}\n",
    "  \\label{eq:32}\n",
    "  \\bo{d}_i'\\bo{r}_i = \\bo{r}_i'\\bo{r}_i.\n",
    "\\end{equation}\n",
    "Using this is (\\ref{eq:31}) gives\n",
    "\\begin{equation}\n",
    "  \\label{eq:33}\n",
    "  \\bo{r}_{i-1}'\\bo{r}_{i-1} = -\\alpha_{i-1}\\bo{d}_{i-1}'\\bo{Ad}_{i-1}.\n",
    "\\end{equation}\n",
    "Using (\\ref{eq:30}) and (\\ref{eq:33}) in (\\ref{eq:28}) gives\n",
    "\\begin{equation}\n",
    "  \\label{eq:34}\n",
    "  \\beta_{i-1} = \\frac{-\\bo{r}_i'\\bo{r}_i}{\\bo{r}_{i-1}'\\bo{r}_{i-1}}.\n",
    "\\end{equation}\n",
    "Finally, using (\\ref{eq:32}) in (\\ref{eq:16}) gives\n",
    "\\begin{equation}\n",
    "  \\label{eq:35}\n",
    "  \\alpha_i = \\frac{-\\bo{r}_i'\\bo{r}_i}{\\bo{d}_i'\\bo{Ad}_i}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preconditioned conjugate gradient method\n",
    "In the PCCG method, the conjugate gradient method is applied to a transformed\n",
    "system of equations. The transformation of the system is based on a matrix\n",
    "$\\mathbf{M}$ that is approximately equal to $\\mathbf{A}$ and is easy to invert.  A detailed\n",
    "explanation of PCCG is given in **An Introduction to the Conjugate Gradient\n",
    "Method Without the Agonizing Pain** by Jonathan Richard Shewchuk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCCG, the solution at iteration $n+1$ is:\n",
    "\\begin{equation}\n",
    "  \\label{eq:36}\n",
    "  \\bo{x}_{n+1} = \\bo{x}_n + \\alpha_n\\bo{d}_n,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  \\label{eq:37}\n",
    "  \\alpha_n = \\frac{-\\bo{r}_n'\\bo{M}^{-1}\\bo{r}_n}\n",
    "            {\\bo{d}_n'\\bo{Ad}_n},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  \\label{eq:38}\n",
    "  \\bo{r}_n = \\bo{Ax}_n - \\bo{b},  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  \\label{eq:39}\n",
    "  \\bo{d}_n = \\bo{M}^{-1}\\bo{r}_n - \\beta_{n-1}\\bo{d}_{n-1}, \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  \\label{eq:40}\n",
    "  \\beta_{n-1} = \\frac{-\\bo{r}_n'\\bo{M}^{-1}\\bo{r}_n}\n",
    "                {\\bo{r}'_{n-1}\\bo{M}^{-1}\\bo{r}_{n-1}}.  \n",
    "\\end{equation}\n",
    "As in the conjugate gradient method, the residual can be computed more\n",
    "efficiently as\n",
    "\\begin{equation}\n",
    "  \\label{eq:41}\n",
    "  \\bo{r}_n = \\bo{r}_{n-1} + \\alpha_{n-1}\\bo{Ad}_{n-1}.  \n",
    "\\end{equation}\n",
    "However, it is recommended that \\eqref{eq:38} is used to every 50 iterations to\n",
    "avoid the accumulation of errors.\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
